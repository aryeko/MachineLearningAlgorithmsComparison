{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from LogisticRegression.LogisticRegression import LogisticRegression\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parseExcelDataSet(filePath):\n",
    "    xl = pd.ExcelFile(filePath)\n",
    "    parsed = xl.parse(xl.sheet_names[0])\n",
    "    data = np.array(parsed)\n",
    "\n",
    "    rows, cols = data.shape\n",
    "\n",
    "    # Classification is in the last column\n",
    "    Y = data[:, cols - 1]\n",
    "    X = data[:, :cols - 1]\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_scores(scores, details, x_values, x_label, title=None, figsize=(16, 9)):\n",
    "    marks = ['r-', 'b-', 'y-', 'g-']\n",
    "    colors = ['red', 'blue', 'yellow', 'green']\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    plt.legend(handles=[mpatches.Patch(color=col, label=det) for col, det in zip(colors, details)])\n",
    "    if title != None:\n",
    "        plt.title(title)\n",
    "\n",
    "    for score, mark in zip(scores, marks):\n",
    "        plt.plot(score, mark)\n",
    "\n",
    "    plt.ylabel(\"Success rate\")\n",
    "    plt.xlabel(x_label)\n",
    "    x_lables = list(x_values[0::np.round(len(x_values) / 10).astype(int)])\n",
    "    x_lables.append(x_values[len(x_values) - 1])\n",
    "    plt.ylim(0.5, 1)\n",
    "    plt.xticks(np.append(np.arange(0, len(x_values), len(x_values) / 10).astype(int), [len(x_values)]), x_lables)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def summarize_model(model, train_x, train_y, test_x, test_y):\n",
    "    train_prediction = model.predict(train_x)\n",
    "    train_errors = float(sum(train_prediction != train_y)) / len(train_y)\n",
    "\n",
    "    test_prediction = model.predict(test_x)\n",
    "    test_errors = float(sum(test_prediction != test_y)) / len(test_y)\n",
    "\n",
    "    return train_errors, test_errors\n",
    "\n",
    "\n",
    "def run_logistic_regression(X_train, y_train, X_test, y_test, alpha, num_of_features):\n",
    "    zero_one_y_train = [0 if c == 'M' else 1 for c in y_train]\n",
    "    zero_one_y_test = [0 if c == 'M' else 1 for c in y_test]\n",
    "\n",
    "    start = time.time()\n",
    "    model = LogisticRegression(X_train, zero_one_y_train, alpha, num_of_features)\n",
    "    iterations_cost_array = model.gradient_decent(200, 0.01)\n",
    "    train_errors, test_errors = summarize_model(model, X_train, zero_one_y_train, X_test, zero_one_y_test)\n",
    "\n",
    "    return 1 - test_errors\n",
    "\n",
    "\n",
    "def run_stochastic_logistic_regression(X_train, y_train, X_test, y_test, alpha, num_of_features):\n",
    "    zero_one_y_train = [0 if c == 'M' else 1 for c in y_train]\n",
    "    zero_one_y_test = [0 if c == 'M' else 1 for c in y_test]\n",
    "\n",
    "    start = time.time()\n",
    "    model = LogisticRegression(X_train, zero_one_y_train, alpha, num_of_features)\n",
    "    iterations_cost_array = model.stochastic_gradient_decent(500, 0.01)\n",
    "    train_errors, test_errors = summarize_model(model, X_train, zero_one_y_train, X_test, zero_one_y_test)\n",
    "\n",
    "    return 1 - test_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_k_fold(X, Y, clf, algorithm, draw_scores=False, k=10):\n",
    "    # k-fold cross validation\n",
    "    # shuffle data\n",
    "    index = [i for i in range(len(Y))]\n",
    "    random.shuffle(index)\n",
    "    X = X[index, :]\n",
    "    Y = Y[index]\n",
    "\n",
    "    # k-fold\n",
    "    kfold = k\n",
    "    foldSize = int(len(Y) / kfold)\n",
    "\n",
    "    # arrage to store training and testing error\n",
    "    test_score = []\n",
    "    train_score = []\n",
    "    allIndex = range(len(Y))\n",
    "\n",
    "    for k in range(0, kfold):\n",
    "        test_indexes = range((foldSize * k), foldSize * (k + 1))\n",
    "        train_indexes = list(set(allIndex) - set(test_indexes))\n",
    "\n",
    "        train_x, train_y = X[train_indexes, :], Y[train_indexes]\n",
    "        test_x, test_y = X[test_indexes, :], Y[test_indexes]\n",
    "\n",
    "        clf.fit(train_x, train_y)\n",
    "        train_score.append(clf.score(train_x, train_y))\n",
    "        test_score.append(clf.score(test_x, test_y))\n",
    "\n",
    "    if draw_scores:\n",
    "        if algorithm == 'SVM':\n",
    "            plot_scores([train_score, test_score], [\"train_score\", \"test_score\"], range(kfold), \"k fold\",\n",
    "                        'SVM: %s kernel with c= %s' % (clf.kernel, clf.C), (4, 2.25))\n",
    "        if algorithm == 'Neural Network':\n",
    "            plot_scores([train_score, test_score], [\"train_score\", \"test_score\"], range(kfold), \"k fold\",\n",
    "                        'Neural Network: %s solver with alpha= %s' % (clf.solver, clf.alpha), (6, 3.5))\n",
    "\n",
    "    return train_score, test_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runKFoldAlgorithm():\n",
    "    X, Y = parseExcelDataSet(\"ExcerciseData\\\\real_project_data.xls\")\n",
    "\n",
    "    print(\"Running 10-Fold cross validation algorithm using SVM model (figures will pop up, close to continue)...\")\n",
    "    C_arr = np.arange(0.1, 2, 0.5)\n",
    "    svm_kernels = [\"linear\", \"rbf\", \"sigmoid\"]\n",
    "    kernels_score = []\n",
    "    for ker in svm_kernels:\n",
    "        ker_score = []\n",
    "        for c in C_arr:\n",
    "            clf = SVC(kernel=ker, C=c, degree=2)\n",
    "            train_score, test_score = run_k_fold(X, Y, clf, 'SVM', draw_scores=True)\n",
    "            ker_score.append(np.mean(test_score))\n",
    "        kernels_score.append(ker_score)\n",
    "\n",
    "    print(\"Running 10-Fold cross validation algorithm using Neural Network model (figures will pop up, close to continue)...\")\n",
    "    alpha_arr = np.arange(0.1, 2, 0.5)\n",
    "    solvers = [\"lbfgs\", \"sgd\", \"adam\"]\n",
    "    scores = []\n",
    "    for solver in solvers:\n",
    "        nn_score = []\n",
    "        for a in alpha_arr:\n",
    "            clf = MLPClassifier(alpha=a, solver=solver)\n",
    "            train_score, test_score = run_k_fold(X, Y, clf, 'Neural Network', draw_scores=True)\n",
    "            nn_score.append(np.mean(test_score))\n",
    "        scores.append(nn_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runAlgorithmComparison():\n",
    "    print(\"\\n\\nRunning 4 machine learning algorithms using different configurations.\"\n",
    "          \"\\nEach algorithm comparison will be illustrated on a detailed graph\\n\")\n",
    "\n",
    "    X, Y = parseExcelDataSet(\"ExcerciseData\\\\real_project_data.xls\")\n",
    "\n",
    "    # preprocess dataset, split into training and test part\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=.4, random_state=42)\n",
    "\n",
    "    print(\"Running SVM...\")\n",
    "    C_arr = np.arange(0.001, 5, 0.05)\n",
    "    svm_kernels = [\"linear\", \"rbf\", \"sigmoid\"]\n",
    "    kernels_score = []\n",
    "    for ker in svm_kernels:\n",
    "        ker_score = []\n",
    "        for c in C_arr:\n",
    "            clf = SVC(kernel=ker, C=c, degree=2)\n",
    "            clf.fit(X_train, y_train)\n",
    "            score = clf.score(X_test, y_test)\n",
    "            ker_score.append(score)\n",
    "        kernels_score.append(ker_score)\n",
    "\n",
    "    plot_scores(kernels_score, svm_kernels, C_arr, \"C values\", \"Running SVM model\\nComparison between 3 different kernels and a range of C values\")\n",
    "\n",
    "    print(\"Running Neural Network...\")\n",
    "    alpha_arr = np.arange(0.0001, 5, 0.05)\n",
    "    solvers = [\"lbfgs\", \"sgd\", \"adam\"]\n",
    "    solvers_details = [\"lbfgs (Newton) solver\", \"sgd (stochastic gradient descent) solver\",\n",
    "                       \"adam (stochastic gradient-based optimizer) solver\"]\n",
    "    scores = []\n",
    "    for solver in solvers:\n",
    "        nn_score = []\n",
    "        for a in alpha_arr:\n",
    "            clf = MLPClassifier(alpha=a, solver=solver)\n",
    "            clf.fit(X_train, y_train)\n",
    "            score = clf.score(X_test, y_test)\n",
    "            nn_score.append(score)\n",
    "        scores.append(nn_score)\n",
    "\n",
    "    plot_scores(scores, solvers_details, alpha_arr, \"alpha values\",\n",
    "                \"Running Neural Network model\\nComparison between 3 different solvers and a range of alpha values\")\n",
    "\n",
    "    print(\"Running Logistic Regression model using batch gradient descent...\")\n",
    "    alpha_arr = np.arange(0.05, 5, 0.05)\n",
    "    num_of_features = [5, 10, 15]\n",
    "    features_detailes = [\"5 features\", \"10 features\", \"15 features\"]\n",
    "    scores = []\n",
    "    for nf in num_of_features:\n",
    "        score = []\n",
    "        for a in alpha_arr:\n",
    "            s = run_logistic_regression(X_train, y_train, X_test, y_test, a, nf)\n",
    "            score.append(s)\n",
    "        scores.append(score)\n",
    "\n",
    "    plot_scores(scores, features_detailes, alpha_arr, \"alpha values\", \"Running Logistic Regression model using BGD\\nComparison between 5/10/15 most influences features and range of alpha values\")\n",
    "\n",
    "    print(\"Running Logistic Regression model using stochastic gradient descent...\")\n",
    "    alpha_arr = np.arange(0.05, 5, 0.05)\n",
    "    num_of_features = [5, 10, 15]\n",
    "    features_detailes = [\"5 features\", \"10 features\", \"15 features\"]\n",
    "    scores = []\n",
    "    for nf in num_of_features:\n",
    "        score = []\n",
    "        for a in alpha_arr:\n",
    "            s = run_stochastic_logistic_regression(X_train, y_train, X_test, y_test, a, nf)\n",
    "            score.append(s)\n",
    "        scores.append(score)\n",
    "\n",
    "    plot_scores(scores, features_detailes, alpha_arr, \"alpha values\", \"Running Logistic Regression model using SGD\\nComparison between 5/10/15 most influences features and range of alpha values\")\n",
    "    print(\"Comparison finished successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runBestAlgorithm(argv):\n",
    "    print(\"\\n\\nRunning SVM algorithm while using 'linear' kernel and C=1.7 parameters as second best algorithm.\"\n",
    "          \"\\nThis algorithm was chose as the best algorithm for the given data-set after running K-Fold cross validation algorithm\")\n",
    "\n",
    "    clf = SVC(kernel=\"linear\", C=1.7, degree=2)\n",
    "    calculateResults(argv, clf, \"SVM\")\n",
    "\n",
    "def calculateResults(argv, clf, algorithm):\n",
    "    X_train, y_train = parseExcelDataSet(\"ExcerciseData\\\\real_project_data.xls\")\n",
    "    X_test, y_test = parseExcelDataSet(argv)\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    score = clf.score(X_test, y_test)\n",
    "    y_prediction = clf.predict(X_test)\n",
    "\n",
    "    index_arr = []\n",
    "    for index, value in enumerate(y_prediction):\n",
    "        if value != y_test[index]:\n",
    "            index_arr.append(index)\n",
    "\n",
    "    print(\"\\nResults for %s where kernel=%s and C=%s:\" % (algorithm, clf.kernel, clf.C))\n",
    "    print(\"1. Your data-set includes\", len(y_test), \"classifications.\", len(y_test) - len(index_arr), \"of them predicted successfully.\")\n",
    "    print(\"2. Total prediction score is: %.2f%%\" % (score * 100))\n",
    "    print(\"3. The missed prediction indexes are:\\n\", index_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "runAlgorithmComparison()\n",
    "runKFoldAlgorithm()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
